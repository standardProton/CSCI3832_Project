{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM MODEL\n",
    "Yufan qian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "csv_file = './datasets/SpamHam/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class dataset():\n",
    "    def __init__(self, csv_file, max):\n",
    "        self.data = pd.read_csv(csv_file, name=['labels','content'])\n",
    "        self.max = 200\n",
    "        self.tokenize = get_tokenize\n",
    "        counter = Counter()\n",
    "        for text in self.data['content']:\n",
    "            min_fre = 1\n",
    "            self.vocab = vocab(Counter)\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.data)\n",
    "    def __getvob__(self,index):\n",
    "        label = self.data.iloc[index]['labels']\n",
    "        content = content[:self.max]\n",
    "        content_tokens = [self.vocab[token] for token in self.token(content)]\n",
    "        if len(content_tokens) < self.max:\n",
    "            content_tokens += [0]*(self.max - len(content_tokens))\n",
    "        else:\n",
    "            content_tokens = content_tokens[:self.max]\n",
    "        \n",
    "        content_tensor = torch.tensor(content_tokens, dtype = torch.long)\n",
    "        \n",
    "        labels_tensor = torch.tensor(1 if labels == 'spam' else 0, dtype = torch.long)\n",
    "        \n",
    "        return {'content': content_tensor, 'labels': labels_tensor}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#adapted from Homework 3\n",
    "def train_lstm_classification(model, train_dataset, valid_dataset, epochs=10, batch_size=32, learning_rate=.001, print_frequency=25):\n",
    "\n",
    "    criteria = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "    epochs = epochs\n",
    "    batch_size = batch_size\n",
    "    print_frequency = print_frequency\n",
    "\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "    print('Total train batches: {}'.format(train_dataset.__len__() / batch_size))\n",
    "\n",
    "    best_accuracy = 0.0\n",
    "    best_model_sd = None\n",
    "\n",
    "    for i in range(epochs):\n",
    "        print('### Epoch: ' + str(i+1) + ' ###')\n",
    "    \n",
    "        model.train()\n",
    "\n",
    "        avg_loss = 0\n",
    "\n",
    "        for step, data in enumerate(train_dataloader):\n",
    "\n",
    "            (x, x_lengths), y = data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            model_output = model(x, x_lengths)\n",
    "\n",
    "            loss = criteria(model_output.squeeze(1), y.float())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            if step % print_frequency == (print_frequency - 1):\n",
    "                print('epoch: {} batch: {} loss: {}'.format(\n",
    "                    i,\n",
    "                    step,\n",
    "                    avg_loss / print_frequency\n",
    "                ))\n",
    "                avg_loss = 0\n",
    "\n",
    "        print('Evaluating...')\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            acc = predict(model, valid_dataloader)\n",
    "            if acc > best_accuracy:\n",
    "                best_model_sd = copy.deepcopy(model.state_dict())\n",
    "                best_accuracy = acc\n",
    "\n",
    "    return model.state_dict(), best_model_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    " \n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        #adapted from Homework 3\n",
    "    def forward(self, input_batch, input_lengths):\n",
    "        print('Input batch shape: {}'.format(input_batch.shape))\n",
    "        embedded_input = self.embedding(input_batch)\n",
    "        \n",
    "        print('Embedded input shape: {}'.format(embedded_input.shape))\n",
    "        packed_input = pack_padded_sequence(embedded_input, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "        \n",
    "        # See docs linked below for description of hn.shape\n",
    "        hn_view = hn.view(self.lstm.num_layers, self.num_directions, input_batch.shape[0], self.lstm.hidden_size)               # Reshape hn for clarity -- first dimension now represents each layer (total set by num_lstm_layers)\n",
    "        print('hn_view input shape: {}'.format(hn_view.shape))\n",
    "        \n",
    "        hn_view_last_layer = hn_view[-1]                                                                                        # Taking the last layer for our final LSTM output\n",
    "        print('hn_view_last_layer input shape: {}'.format(hn_view_last_layer.shape))\n",
    "        \n",
    "        hn_cat = torch.cat([hn_view_last_layer[-2, :, :], hn_view_last_layer[-1, :, :]], dim=1)                                 # Each layer has two directions. We want to use both of these vectors, so concatenate them\n",
    "        print('hn_cat input shape: {}'.format(hn_cat.shape))\n",
    "        \n",
    "        hid = self.relu(self.hidden_1(hn_cat))\n",
    "        print('hid input shape: {}'.format(hid.shape))\n",
    "        \n",
    "        output = self.hidden_2(hid)\n",
    "        print('output input shape: {}'.format(output.shape))\n",
    "    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_trigram(trigram_model, trigram_dataset):\n",
    "\n",
    "    criteria = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(trigram_model.parameters())\n",
    "    # optimizer = torch.optim.SGD(trigram_model.parameters(), lr=0.5)\n",
    "\n",
    "    softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    epochs = 3\n",
    "    batch_size = 32\n",
    "    print_frequency = 1000\n",
    "\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(trigram_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        print('### Epoch: ' + str(i+1) + ' ###')\n",
    "\n",
    "        trigram_model.train()\n",
    "        avg_loss = 0\n",
    "\n",
    "        for step, data in enumerate(train_dataloader):\n",
    "\n",
    "            x, y = data\n",
    "\n",
    "            x = x.unsqueeze(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            model_output = trigram_model(x)\n",
    "            model_output_probabilities = softmax(model_output)\n",
    "\n",
    "            loss = criteria(model_output_probabilities.squeeze(1), y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_loss += loss.item()\n",
    "            if step % print_frequency == 1:\n",
    "                print('epoch: {} batch: {} loss: {}'.format(\n",
    "                    i,\n",
    "                    step,\n",
    "                    avg_loss / print_frequency\n",
    "                ))\n",
    "                avg_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model = LSTM(input_size, hidden_size, output_size, num_layers)\n",
    "model, best_model = train_lstm_classification(model, train_dataset, valid_dataset, batch_size=128, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
