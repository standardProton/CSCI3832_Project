{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM MODEL\n",
    "Yufan qian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../3832/glove.6B/glove.6B.50d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m glove_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../3832/glove.6B/glove.6B.50d.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m embeddings_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(glove_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(f):\n\u001b[1;32m      7\u001b[0m         line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../3832/glove.6B/glove.6B.50d.txt'"
     ]
    }
   ],
   "source": [
    "glove_file = '../3832/glove.6B/glove.6B.50d.txt'\n",
    "\n",
    "embeddings_dict = {}\n",
    "\n",
    "with open(glove_file, 'r', encoding='utf8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        line = line.strip().split(' ')\n",
    "        word = line[0]\n",
    "        embed = np.asarray(line[1:], \"float\")\n",
    "\n",
    "        embeddings_dict[word] = embed\n",
    "\n",
    "\n",
    "print('Loaded {} words from glove'.format(len(embeddings_dict)))\n",
    "\n",
    "low = -1.0 / 3\n",
    "high = 1.0 / 3\n",
    "embedding_matrix = np.random.uniform(low=low, high=high, size=(len(embeddings_dict)+1, 50))\n",
    "\n",
    "word2id = {}\n",
    "for i, word in enumerate(embeddings_dict.keys(), 1):\n",
    "\n",
    "    word2id[word] = i                                \n",
    "    embedding_matrix[i] = embeddings_dict[word]      \n",
    "\n",
    "word2id['<pad>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:10\u001b[0;36m\u001b[0m\n\u001b[0;31m    def __len__(self):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "class dataset():\n",
    "     def __init__(self, cvs_file, max):\n",
    "        self.data = pd.read_csv(csv_file, name=['labels','content'])\n",
    "        self.max = 200\n",
    "        self.tokenize = get_tokenize\n",
    "        counter = Counter()\n",
    "        for text in self.data['content']:\n",
    "            min_fre = 1\n",
    "            self.vob = vob(counter)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getvob__(self,index):\n",
    "        label = self.data.iloc[index]['labels']\n",
    "        content = content[:self.max]\n",
    "        content_tokens = [self.vocab[token] for token in self.token(content)]\n",
    "        if len(content_tokens) < self.max:\n",
    "            content_tokens += [0]*(self.max - len(content_tokens))\n",
    "        else:\n",
    "            content_tokens = content_tokens[:self.max]\n",
    "        \n",
    "        content_tensor = torch.tensor(content_tokens)\n",
    "        \n",
    "        labels_tensor = torch.tensor(1 if labels == 'spam' else 0)\n",
    "        \n",
    "        return {'content': content_tensor, 'labels': labels_tensor}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_trigram(trigram_model, trigram_dataset):\n",
    "\n",
    "    criteria = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(trigram_model.parameters())\n",
    "\n",
    "    softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    epochs = 3\n",
    "    batch_size = 32\n",
    "    print_frequency = 1000\n",
    "\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(trigram_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        print('### Epoch: ' + str(i+1) + ' ###')\n",
    "\n",
    "        trigram_model.train()\n",
    "        avg_loss = 0\n",
    "\n",
    "        for step, data in enumerate(train_dataloader):\n",
    "\n",
    "            x, y = data\n",
    "\n",
    "            x = x.unsqueeze(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            model_output = trigram_model(x)\n",
    "            model_output_probabilities = softmax(model_output)\n",
    "\n",
    "            loss = criteria(model_output_probabilities.squeeze(1), y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_loss += loss.item()\n",
    "            if step % print_frequency == 1:\n",
    "                print('epoch: {} batch: {} loss: {}'.format(\n",
    "                    i,\n",
    "                    step,\n",
    "                    avg_loss / print_frequency\n",
    "                ))\n",
    "                avg_loss = 0\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss vs Step')\n",
    "    plt.show()\n",
    "\n",
    "    return [model, best_model_sd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#adapted from Homework 3\n",
    "def train_lstm_classification(model, train_dataset, valid_dataset, epochs=10, batch_size=32, learning_rate=.001, print_frequency=25):\n",
    "\n",
    "    criteria = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "    epochs = epochs\n",
    "    batch_size = batch_size\n",
    "    print_frequency = print_frequency\n",
    "\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "    print('Total train batches: {}'.format(train_dataset.__len__() / batch_size))\n",
    "\n",
    "    best_accuracy = 0.0\n",
    "    best_model_sd = None\n",
    "\n",
    "    for i in range(epochs):\n",
    "        print('### Epoch: ' + str(i+1) + ' ###')\n",
    "    \n",
    "        model.train()\n",
    "\n",
    "        avg_loss = 0\n",
    "\n",
    "        for step, data in enumerate(train_dataloader):\n",
    "\n",
    "            (x, x_lengths), y = data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            model_output = model(x, x_lengths)\n",
    "\n",
    "            loss = criteria(model_output.squeeze(1), y.float())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            if step % print_frequency == (print_frequency - 1):\n",
    "                print('epoch: {} batch: {} loss: {}'.format(\n",
    "                    i,\n",
    "                    step,\n",
    "                    avg_loss / print_frequency\n",
    "                ))\n",
    "                avg_loss = 0\n",
    "\n",
    "        print('Evaluating...')\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            acc = predict(model, valid_dataloader)\n",
    "            if acc > best_accuracy:\n",
    "                best_model_sd = copy.deepcopy(model.state_dict())\n",
    "                best_accuracy = acc\n",
    "\n",
    "    return model.state_dict(), best_model_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    " \n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        #adapted from Homework 3\n",
    "    def forward(self, input_batch, input_lengths):\n",
    "        print('Input batch shape: {}'.format(input_batch.shape))\n",
    "        embedded_input = self.embedding(input_batch)\n",
    "        \n",
    "        print('Embedded input shape: {}'.format(embedded_input.shape))\n",
    "        packed_input = pack_padded_sequence(embedded_input, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "        \n",
    "        hn_view = hn.view(self.lstm.num_layers, self.num_directions, input_batch.shape[0], self.lstm.hidden_size)               # Reshape hn for clarity -- first dimension now represents each layer (total set by num_lstm_layers)\n",
    "        print('hn_view input shape: {}'.format(hn_view.shape))\n",
    "        \n",
    "        hn_view_last_layer = hn_view[-1]                                                                                        # Taking the last layer for our final LSTM output\n",
    "        print('hn_view_last_layer input shape: {}'.format(hn_view_last_layer.shape))\n",
    "        \n",
    "        hn_cat = torch.cat([hn_view_last_layer[-2, :, :], hn_view_last_layer[-1, :, :]], dim=1)                                 # Each layer has two directions. We want to use both of these vectors, so concatenate them\n",
    "        print('hn_cat input shape: {}'.format(hn_cat.shape))\n",
    "        \n",
    "        hid = self.relu(self.hidden_1(hn_cat))\n",
    "        print('hid input shape: {}'.format(hid.shape))\n",
    "        \n",
    "        output = self.hidden_2(hid)\n",
    "        print('output input shape: {}'.format(output.shape))\n",
    "    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute '__len__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m LSTM(\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model, best_model \u001b[38;5;241m=\u001b[39m train_lstm_classification(model, \u001b[38;5;241m300\u001b[39m, \u001b[38;5;241m200\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[20], line 16\u001b[0m, in \u001b[0;36mtrain_lstm_classification\u001b[0;34m(model, train_dataset, valid_dataset, epochs, batch_size, learning_rate, print_frequency)\u001b[0m\n\u001b[1;32m     13\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m valid_dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(valid_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal train batches: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(train_dataset\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__len__\u001b[39m() \u001b[38;5;241m/\u001b[39m batch_size))\n\u001b[1;32m     18\u001b[0m best_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     19\u001b[0m best_model_sd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute '__len__'"
     ]
    }
   ],
   "source": [
    "model = LSTM(200, 100, 16, 8)\n",
    "model, best_model = train_lstm_classification(model, 300, 200, batch_size=128, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
