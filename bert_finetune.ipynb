{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2 and Bert finetuning comparisions\n",
    "\n",
    "\n",
    "Aaron Semones\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "import os \n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import io\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2Config, GPT2ForSequenceClassification, GPT2Tokenizer, AutoTokenizer,  AutoModelForSequenceClassification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libaries \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    " # formatting function for phishing dataset\n",
    "def remove_doublequotes(file_dir): # \n",
    "    raw_file_str = ''\n",
    "    with open(file_dir, 'r', encoding='utf-8') as f:\n",
    "        raw_file_str = f.read().replace('\"\"', '\"')\n",
    "    with open(file_dir, 'w', encoding='utf-8') as f:\n",
    "        f.write(raw_file_str)\n",
    "        \n",
    "#partition a dictionary\n",
    "def split_dict (dict1, index):\n",
    "    dict1c = dict1\n",
    "    dict1 = dict(list(dict1c.items())[index:])\n",
    "    dict2 = dict(list(dict1c.items())[:index])\n",
    "    return dict1, dict2\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misc. helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") #sets your device. Code will run at a snails pace without gpu\n",
    "\n",
    "\n",
    "\n",
    "class Datasets(Dataset):\n",
    "    def __init__ (self, testpath=None, Emails = None, size =None, final_data = None , data_processed=False): \n",
    "        '''testpath is the path of your csv file, emails is a bool for handling oddities with phising ds\n",
    "        size is the desired num_entries, final_data is for turning already pre-processed data into a dataset for loading (ie validiation set),\n",
    "        dataprocessed is the bool controlling that functionalty\n",
    "        \n",
    "        \n",
    "        Members:\n",
    "        \n",
    "        length is number of examples\n",
    "        test_set is a dict of the entire set\n",
    "        set_labels is a list of the labels\n",
    "        set_text is a list of the text\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        if (data_processed):\n",
    "            self.length = len(final_data)\n",
    "            self.test_set = final_data\n",
    "            self.set_labels = [self.test_set[x] for x in self.test_set]\n",
    "            self.set_text = list (self.test_set.keys())\n",
    "            return\n",
    "        \n",
    "        if Emails == False:\n",
    "            remove_doublequotes(testpath)\n",
    "        \n",
    "        \n",
    "        if (size):\n",
    "            self.test_set = pd.read_csv(testpath, nrows = size)\n",
    "        else: \n",
    "            self.test_set = pd.read_csv(testpath)\n",
    " \n",
    "        if (Emails == True):\n",
    "            self.test_set = self.test_set.set_index('text')['label'].to_dict()\n",
    "        \n",
    "        else:\n",
    "            self.test_set['label'] = self.test_set ['label'].apply(lambda x : 2 - x)\n",
    "            print(self.test_set.head())\n",
    "            self.test_set =  self.test_set.set_index('text')['label'].to_dict() \n",
    "            \n",
    "        self.length = len (self.test_set)\n",
    "        \n",
    "        self.set_labels = [self.test_set[x] for x in self.test_set]\n",
    "        self.set_text = list (self.test_set.keys())\n",
    "        \n",
    "        return\n",
    "    #homework 3 inspired validation data spilt function\n",
    "    def split(self, ratio = .8 ):\n",
    "        index = int(ratio*self.length)\n",
    "        \n",
    "        split,self.test_set = split_dict(self.test_set, index)\n",
    "        \n",
    "        self.set_labels = self.set_labels[:index]\n",
    "        self.set_text = self.set_text[:index]\n",
    "        self.length = len(self.test_set)\n",
    "        \n",
    "        return split\n",
    "    #functions required by pytorch for handling data\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return {'text': self.set_text[index], 'label': self.set_labels[index]}\n",
    "    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primary dataset classes for handling data and loading it into pytorch. Code written to be reuseable, allowing for easy testing of different models\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' tokenizer class to allow easy swapping of tokenizers\n",
    "params: use_tokenizer is the tokenizer function of choice\n",
    "max: the maxiumum sequence length. tokenizer truncates based off this'''\n",
    "\n",
    "class _tokenize(object):\n",
    "    def __init__(self,  use_tokenizer, max=512):\n",
    "         self.use_tokenizer = use_tokenizer\n",
    "         self.max_sequence_len =max\n",
    "    #basically just calls the tokenizer, returning embeddings  dict    \n",
    "    def __call__(self, data):\n",
    "        text= [x['text'] for x in data]\n",
    "        label = [x ['label'] for x in data]\n",
    "        \n",
    "        \n",
    "        embeddings = self.use_tokenizer(text=text, return_tensors = \"pt\", padding = True, truncation= True, max_length = self.max_sequence_len)\n",
    "        embeddings.update({'labels' : torch.tensor(label)})\n",
    "        return embeddings\n",
    "#helper to solve for accuracy, true pos, true neg, false pos, false neg\n",
    "# takes in the actual labels and a series of predictions\n",
    "#outputs array of stats\n",
    "def calculate_stats(labels, predictions):\n",
    "    acc = 0.0\n",
    "    fp =0.0\n",
    "    fn = 0.0\n",
    "    tp =0.0\n",
    "    tn = 0.0\n",
    "    size = len(labels)\n",
    "    counter = 0\n",
    "    \n",
    "    for x in labels:\n",
    "        if x == 1 and predictions[counter] == 1:\n",
    "            tp+=1\n",
    "            acc +=1\n",
    "        elif x == 0 and predictions[counter] == 0:\n",
    "            tn+=1\n",
    "            acc+=1\n",
    "        elif x == 1:\n",
    "            fp  +=1\n",
    "        elif x == 0:\n",
    "            fn +=1 \n",
    "        counter +=1\n",
    "            \n",
    "    return [acc/size, tp/size, tn/size, fp/size, fn/size]\n",
    "#evaluates the model \n",
    "def test (model, data, device, ):\n",
    "    print (\"Evaluating\")\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    predictions = []\n",
    "    labels= []\n",
    "   \n",
    "    model.eval()\n",
    "    for batch in tqdm (data, total=len(data)):\n",
    "        labels += batch['labels'].numpy().flatten().tolist()\n",
    "\n",
    "        batch = {i:j.type(torch.long).to(device) for i,j in batch.items()}\n",
    "        with torch.no_grad():\n",
    "\n",
    "           \n",
    "            model_out = model(**batch)\n",
    "            loss,logits =model_out[:2]\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "            logits = logits.detach().cpu().numpy() \n",
    "        \n",
    "            predictions  += logits.argmax(axis = -1).flatten().tolist()\n",
    "    total_loss = total_loss/len(data)\n",
    "    \n",
    "  \n",
    "    stats= calculate_stats (labels, predictions)        \n",
    "       \n",
    "    return total_loss, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer class and helpers for our training loop + eval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Training loop  \n",
    "Params: model, data (your training data), valid (your valid data), optimizer, scheduler, device, epochs'''\n",
    "\n",
    "\n",
    "\n",
    "def train(model, data, valid, optimizer, scheduler, device, epochs=1):\n",
    "    \n",
    "  \n",
    "    avg_loss_per_epoch = list ()\n",
    "    acc_t = []\n",
    "    tp_t = []\n",
    "    tn_t = []\n",
    "    fp_t = []\n",
    "    fn_t = []\n",
    "    \n",
    "    acc_v = []\n",
    "    tp_v = []\n",
    "    tn_v = []\n",
    "    fp_v = []\n",
    "    fn_v = []\n",
    "    \n",
    "    v_loss_t = list ()\n",
    "    for i in range(epochs):\n",
    "        print (\"Training \" ,i, \" Epoch\")\n",
    "        total_loss = 0\n",
    "        predictions = []\n",
    "        labels = []\n",
    "        v_loss = []\n",
    "        model.train()\n",
    "        for batch in tqdm (data, total=len(data)):\n",
    "            labels += batch['labels'].numpy().flatten().tolist()\n",
    "            batch = {i:j.type(torch.long).to(device) for i,j in batch.items()}\n",
    "\n",
    "            model.zero_grad()\n",
    "        \n",
    "        \n",
    "            model_out = model(**batch)\n",
    "        \n",
    "            loss_obj, logits = model_out[:2]\n",
    "    \n",
    "        \n",
    "            total_loss += loss_obj.item()\n",
    "        \n",
    "            loss_obj.backward()\n",
    "        \n",
    "        \n",
    "        \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            \n",
    "            logits = logits.detach().cpu().numpy()\n",
    "        \n",
    "            predictions  += logits.argmax(axis = -1).flatten().tolist()\n",
    "  \n",
    "        stats= calculate_stats (labels, predictions)\n",
    "        \n",
    "        avg_loss_per_epoch.append(total_loss/len(data))\n",
    "        \n",
    "        print (\"training accuracy for epoch \", i ,\": \", stats[0])\n",
    "        print (\"training loss for epoch \", i ,\": \",total_loss/len(data))\n",
    "        acc_t.append(stats[0])\n",
    "        tp_t.append(stats[1])\n",
    "        tn_t.append(stats[2])\n",
    "        fp_t.append(stats[3])\n",
    "        fn_t.append(stats[4])\n",
    "        \n",
    "  \n",
    "        v_loss, stats_v = test(model, valid, device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        acc_v.append(stats_v[0])\n",
    "        tp_v.append(stats_v[1])\n",
    "        tn_v.append(stats_v[2])\n",
    "        fp_v.append(stats_v[3])\n",
    "        fn_v.append(stats_v[4])\n",
    "        \n",
    "        v_loss_t.append(v_loss)\n",
    "        \n",
    "        print (\"eval accuracy for epoch \", i ,\": \",stats_v[0])\n",
    "        print (\"eval loss for epoch \", i ,\": \",v_loss)\n",
    "      \n",
    "    t_stats = [avg_loss_per_epoch, acc_t, tp_t, tn_t, fp_t, fn_t]\n",
    "    v_stats = [v_loss_t, acc_v, tp_v, tn_v, fp_v, fn_v]\n",
    "    return  t_stats, v_stats\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tokenzier = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\\ngpt2 = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\\n\\ngpt2.to(device)\\nprint (device,\"being_used\")\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = 'distilgpt2'\n",
    "config = GPT2Config.from_pretrained(pretrained_model_name_or_path = model, num_labels =2, fp16 = True, num_workers = 4)\n",
    "tokenzier = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path = model)\n",
    "tokenzier.padding_side = \"left\"\n",
    "tokenzier.pad_token = '50256'\n",
    "gpt2 = GPT2ForSequenceClassification.from_pretrained(pretrained_model_name_or_path = model, config = config )\n",
    "gpt2.resize_token_embeddings(len(tokenzier))\n",
    "gpt2.config.pad_token_id = gpt2.config.eos_token_id\n",
    "gpt2.to(device)\n",
    "\n",
    "'''tokenzier = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "gpt2 = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "gpt2.to(device)\n",
    "print (device,\"being_used\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inializing our first model, gpt2. gpt2 is a decoder based transformer, which makes it superior at text generation, but generally worse at classification tasks. I used distilgpt2, as gpt2-medium was 500mb too large for my gpus memory. Feel free to go up in complexity if you can run it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baby_spam_test = Datasets(testpath='./datasets/SpamHam/test.csv' ,Emails=True, size = 2000)\n",
    "baby_spam_train = Datasets(testpath='./datasets/SpamHam/train.csv' ,Emails=True, size = 2000)\n",
    "baby_spam_valid = Datasets(final_data = baby_spam_train.split(), data_processed=True)\n",
    "gpt_tokenizer = _tokenize(tokenzier)\n",
    "\n",
    "baby_spam_train_dataloader = DataLoader(baby_spam_train, batch_size= 8, shuffle=True, collate_fn = gpt_tokenizer)\n",
    "baby_spam_valid_dataloader = DataLoader(baby_spam_valid, batch_size=8, shuffle=False, collate_fn = gpt_tokenizer)\n",
    "baby_spam_test_dataloader =  DataLoader(baby_spam_test, batch_size= 8, shuffle=False, collate_fn= gpt_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using a baby dataset for the inital comparisions between the models. Batch size is 'only' 8, as that was the highest I could go without graident checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(gpt2.parameters(), lr= 2e-5, eps = 1e-8)\n",
    "steps = len(baby_spam_train_dataloader)\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0 , num_training_steps = steps)\n",
    "\n",
    "predictions, loss = train(gpt2, baby_spam_train_dataloader, baby_spam_valid_dataloader, optimizer, scheduler, device,3)\n",
    "\n",
    "v_loss, stats_test = test(gpt2, baby_spam_test_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training printouts. eval is broken for some reason, only records the first evaluation and loss. Final printouts from the test set are below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"acc:\", stats_test[0])\n",
    "print (\"True pos\", stats_test[1])\n",
    "print (\"True neg\", stats_test[2])\n",
    "print (\"False pos\", stats_test[3])\n",
    "print (\"False neg\", stats_test[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT2 delivers a respectable test set average on the babyset. 75% accuracy with a 20% false positive rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                               text\n",
      "0      0                       http://minsotc.alania.gov.ru\n",
      "1      0                       http://www.freejavaguide.com\n",
      "2      1  http://yeneliswa.co.za/moods/bankofamerica/7dd...\n",
      "3      0  https://victordahdalehfoundation.com/programme...\n",
      "4      0          http://camphhsi.com/product/list_947.html\n",
      "   label                                               text\n",
      "0      0                        https://blog.sockpuppet.us/\n",
      "1      0                  https://blog.apiki.com/seguranca/\n",
      "2      1  http://autoecole-lauriston.com/a/T0RVd056QXlNe...\n",
      "3      1  http://chinpay.site/index.html?hgcFSE@E$Z*DFcG...\n",
      "4      0  http://www.firstfivenebraska.org/blog/article/...\n"
     ]
    }
   ],
   "source": [
    "baby_phish_test = Datasets('./datasets/PhishingURLs/test.csv' ,False, size=2000)\n",
    "baby_phish_train = Datasets('./datasets/PhishingURLs/train.csv' ,False, size=2000)\n",
    "baby_phish_valid = Datasets(final_data = baby_phish_train.split(), data_processed=True)\n",
    "\n",
    "baby_phish_train_dataloader = DataLoader(baby_phish_train, batch_size= 8, shuffle=True, collate_fn = gpt_tokenizer)\n",
    "baby_phish_valid_dataloader = DataLoader(baby_phish_valid, batch_size=8, shuffle=False, collate_fn = gpt_tokenizer)\n",
    "baby_phish_test_dataloader =  DataLoader(baby_phish_test, batch_size=8, shuffle=False, collate_fn= gpt_tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "phising datasets now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(gpt2.parameters(), lr= 2e-5, eps = 1e-8)\n",
    "steps = len(baby_phish_train_dataloader)\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0 , num_training_steps = steps)\n",
    "\n",
    "predictions, loss = train(gpt2, baby_phish_train_dataloader, baby_phish_valid_dataloader, optimizer, scheduler, device,3)\n",
    "\n",
    "v_loss, stats_test = test(gpt2, baby_phish_test_dataloader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"acc:\", stats_test[0])\n",
    "print (\"True pos\", stats_test[1])\n",
    "print (\"True neg\", stats_test[2])\n",
    "print (\"False pos\", stats_test[3])\n",
    "print (\"False neg\", stats_test[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar preformance on the phishing baby dataset. 80% accuracy with an 13% false postive rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda being_used\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "berto = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert = AutoModelForSequenceClassification.from_pretrained(\"C:/Users/asemo/CSCI 3832/CSCI3832_Project/models\")\n",
    "\n",
    "bert.to(device)\n",
    "\n",
    "print (device,\"being_used\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time for bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'baby_spam_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m bertoken \u001b[38;5;241m=\u001b[39m _tokenize(berto)\n\u001b[1;32m----> 3\u001b[0m baby_spam_train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(baby_spam_train, batch_size\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn \u001b[38;5;241m=\u001b[39m bertoken)\n\u001b[0;32m      4\u001b[0m baby_spam_valid_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(baby_spam_valid, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, collate_fn \u001b[38;5;241m=\u001b[39m bertoken)\n\u001b[0;32m      5\u001b[0m baby_spam_test_dataloader \u001b[38;5;241m=\u001b[39m  DataLoader(baby_spam_test, batch_size\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39m bertoken)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'baby_spam_train' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "bertoken = _tokenize(berto)\n",
    "\n",
    "baby_spam_train_dataloader = DataLoader(baby_spam_train, batch_size= 8, shuffle=True, collate_fn = bertoken)\n",
    "baby_spam_valid_dataloader = DataLoader(baby_spam_valid, batch_size=8, shuffle=False, collate_fn = bertoken)\n",
    "baby_spam_test_dataloader =  DataLoader(baby_spam_test, batch_size= 8, shuffle=False, collate_fn= bertoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.AdamW(bert.parameters(), lr= 2e-5, eps = 1e-8)\n",
    "steps = len(baby_spam_train_dataloader)\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0 , num_training_steps = steps)\n",
    "\n",
    "predictions, loss = train(bert, baby_spam_train_dataloader, baby_spam_valid_dataloader, optimizer, scheduler, device,3)\n",
    "\n",
    "v_loss, stats_test = test(bert, baby_spam_test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"acc:\", stats_test[0])\n",
    "print (\"True pos\", stats_test[1])\n",
    "print (\"True neg\", stats_test[2])\n",
    "print (\"False pos\", stats_test[3])\n",
    "print (\"False neg\", stats_test[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bert blows gpt2 out of the water with a 92% acc and a 5% false postive rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baby_phish_train_dataloader = DataLoader(baby_phish_train, batch_size= 8, shuffle=True, collate_fn = bertoken)\n",
    "baby_phish_valid_dataloader = DataLoader(baby_phish_valid, batch_size=8, shuffle=False, collate_fn = bertoken)\n",
    "baby_phish_test_dataloader =  DataLoader(baby_phish_test, batch_size=8, shuffle=False, collate_fn= bertoken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for phishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(bert.parameters(), lr= 2e-5, eps = 1e-8)\n",
    "steps = len(baby_phish_train_dataloader)\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0 , num_training_steps = steps)\n",
    "\n",
    "predictions, loss = train(bert, baby_phish_train_dataloader, baby_spam_valid_dataloader, optimizer, scheduler, device,3)\n",
    "\n",
    "v_loss, stats_test = test(bert, baby_phish_test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"acc:\", stats_test[0])\n",
    "print (\"True pos\", stats_test[1])\n",
    "print (\"True neg\", stats_test[2])\n",
    "print (\"False pos\", stats_test[3])\n",
    "print (\"False neg\", stats_test[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another strong preformance by bert, with a 89% acc and a 5% false postive. Conclusion: Bert is by far the better model when compared with a slighter smaller sized GPT-2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have found the best preforming model, lets try a run with the fulldatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spam_test = Datasets(testpath='C:/Users/asemo/CSCI 3832/datasets/SpamHam/test.csv' ,Emails=True)\n",
    "spam_train = Datasets(testpath='C:/Users/asemo/CSCI 3832/datasets/SpamHam/train.csv' ,Emails=True)\n",
    "spam_valid = Datasets(final_data = spam_train.split(), data_processed=True)\n",
    "\n",
    "\n",
    "spam_train_dataloader = DataLoader(spam_train, batch_size= 8, shuffle=True, collate_fn = bertoken)\n",
    "spam_valid_dataloader = DataLoader(spam_valid, batch_size=8, shuffle=False, collate_fn = bertoken)\n",
    "spam_test_dataloader =  DataLoader(spam_test, batch_size= 8, shuffle=False, collate_fn= bertoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(bert.parameters(), lr= 2e-5, eps = 1e-8)\n",
    "steps = len(spam_train_dataloader)\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0 , num_training_steps = steps)\n",
    "\n",
    "predictions, loss = train(bert, spam_train_dataloader, spam_valid_dataloader, optimizer, scheduler, device,3)\n",
    "\n",
    "v_loss, stats_test = test(bert, spam_test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"acc:\", stats_test[0])\n",
    "print (\"True pos\", stats_test[1])\n",
    "print (\"True neg\", stats_test[2])\n",
    "print (\"False pos\", stats_test[3])\n",
    "print (\"False neg\", stats_test[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert.save_pretrained('../models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertoken = _tokenize(berto)\n",
    "phish_test = Datasets('C:/Users/asemo/CSCI 3832/datasets/PhishingURLs/test.csv' ,False)\n",
    "phish_train = Datasets('C:/Users/asemo/CSCI 3832/datasets/PhishingURLs/train.csv' ,False)\n",
    "phish_valid = Datasets(final_data = phish_train.split(), data_processed=True)\n",
    "\n",
    "phish_train_dataloader = DataLoader(phish_train, batch_size= 4, shuffle=True, collate_fn = bertoken)\n",
    "phish_valid_dataloader = DataLoader(phish_valid, batch_size=4, shuffle=False, collate_fn = bertoken)\n",
    "phish_test_dataloader =  DataLoader(phish_test, batch_size=4, shuffle=False, collate_fn= bertoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 70.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9333333333333333\n",
      "True pos 0.5\n",
      "True neg 0.43333333333333335\n",
      "False pos 0.06666666666666667\n",
      "False neg 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bertoken = _tokenize(berto)\n",
    "homebrew_ds = Datasets(\"C:/Users/asemo/CSCI 3832/CSCI3832_Project/HomebrewDataset.csv\", Emails=True)\n",
    "homebrew_test = DataLoader(homebrew_ds, batch_size = 1, shuffle = False, collate_fn=bertoken)\n",
    "optimizer = torch.optim.AdamW(bert.parameters(), lr= 2e-5, eps= 1e-8)\n",
    "steps = len(homebrew_test)\n",
    "v_loss, stats_test3 = test(bert, homebrew_test, device)\n",
    "print (\"acc:\", stats_test3[0])\n",
    "print (\"True pos\", stats_test3[1])\n",
    "print (\"True neg\", stats_test3[2])\n",
    "print (\"False pos\", stats_test3[3])\n",
    "print (\"False neg\", stats_test3[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  0  Epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128000/128000 [2:33:06<00:00, 13.93it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy for epoch  0 :  0.9719843202818755\n",
      "training loss for epoch  0 :  0.13220235812806158\n",
      "Evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32000/32000 [08:43<00:00, 61.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval accuracy for epoch  0 :  0.981859375\n",
      "eval loss for epoch  0 :  0.09004282037388169\n",
      "Evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [10:52<00:00, 61.26it/s]\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(bert.parameters(), lr= 2e-5, eps = 1e-8)\n",
    "steps = len(phish_train_dataloader)\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0 , num_training_steps = steps)\n",
    "\n",
    "predictions, loss = train(bert, phish_train_dataloader,  phish_valid_dataloader, optimizer, scheduler, device,1)\n",
    "\n",
    "v_loss, stats_test = test(bert, phish_test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.98195625\n",
      "True pos 0.4890125\n",
      "True neg 0.49294375\n",
      "False pos 0.0109875\n",
      "False neg 0.00705625\n"
     ]
    }
   ],
   "source": [
    "print (\"acc:\", stats_test[0])\n",
    "print (\"True pos\", stats_test[1])\n",
    "print (\"True neg\", stats_test[2])\n",
    "print (\"False pos\", stats_test[3])\n",
    "print (\"False neg\", stats_test[4])\n",
    "\n",
    "\n",
    "for i in range (len(stats_test)):\n",
    "    stats_test[i] = stats_test[i]*len(phish_test_dataloader)\n",
    "    \n",
    "precision = stats_test[2]/(stats_test[2]+ stats_test[4])\n",
    "recall = stats_test[2]/ (stats_test[2]+ stats_test[3])\n",
    "fone= (2*precision*recall)/(precision+recall)\n",
    "print (fone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7266/7266 [20:26<00:00,  5.93it/s]\n"
     ]
    }
   ],
   "source": [
    "v_loss, stats_test = test(bert, spam_test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9882664647993944\n",
      "True pos 0.4675349253320487\n",
      "True neg 0.5207315394673456\n",
      "False pos 0.009152845640355104\n",
      "False neg 0.002580689560250499\n",
      "0.9888591217982228\n"
     ]
    }
   ],
   "source": [
    "print (\"acc:\", stats_test[0])\n",
    "print (\"True pos\", stats_test[1])\n",
    "print (\"True neg\", stats_test[2])\n",
    "print (\"False pos\", stats_test[3])\n",
    "print (\"False neg\", stats_test[4])\n",
    "\n",
    "\n",
    "\n",
    "for i in range (len(stats_test)):\n",
    "    stats_test[i] = stats_test[i]*len(spam_test_dataloader)\n",
    "    \n",
    "precision = stats_test[2]/(stats_test[2]+ stats_test[4])\n",
    "recall = stats_test[2]/ (stats_test[2]+ stats_test[3])\n",
    "fone= (2*precision*recall)/(precision+recall)\n",
    "print (fone)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
