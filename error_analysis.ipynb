{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67d31de3-8213-4c36-9984-62929c0f57b0",
   "metadata": {},
   "source": [
    "# Error Analysis\n",
    "\n",
    "Alex McDonald\n",
    "\n",
    "In this notebook, we will load the models and examine the precise examples that the models failed on. We will focus more on the false negative examples, as the model falsely declaring a malicious email to be safe is more of a concern.\n",
    "\n",
    "First, let's load the relevant code from transformer_train.ipynb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8ce5ca0-d9db-46ad-83de-f53c6c606bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 -0.48336 0.1464 -0.37304 0.34577 0.052041 0.44946 -0.46971 0.02628 -0.54155 -0.15518 -0.14107 -0.039722 0.28277 0.14393 0.23464 -0.31021 0.086173 0.20397 0.52624 0.17164 -0.082378 -0.71787 -0.41531 0.20335 -0.12763 0.41367 0.55187 0.57908 -0.33477 -0.36559 -0.54857 -0.062892 0.26584 0.30205 0.99775 -0.80481 -3.0243 0.01254 -0.36942 2.2167 0.72201 -0.24978 0.92136 0.034514 0.46745 1.1079 -0.19358 -0.074575 0.23353 -0.052062 -0.22044 0.057162 -0.15806 -0.30798 -0.41625 0.37972 0.15006 -0.53212 -0.2055 -1.2526 0.071624 0.70565 0.49744 -0.42063 0.26148 -1.538 -0.30223 -0.073438 -0.28312 0.37104 -0.25217 0.016215 -0.017099 -0.38984 0.87424 -0.72569 -0.51058 -0.52028 -0.1459 0.8278 0.27062\n",
      "\n",
      "Loaded 400000 words from glove\n",
      "0\n",
      "[-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
      "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
      " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
      " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
      " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
      "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
      "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
      " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
      "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
      "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
      "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
      " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
      " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
      " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
      "  0.8278    0.27062 ]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Load the vocabulary space, adapted from Homework 2\n",
    "glove_file = \"./datasets/glove.6B.100d.txt\" #or 50d\n",
    "\n",
    "embeddings_dict = {}\n",
    "\n",
    "with open(glove_file, 'r', encoding='utf8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == 0:\n",
    "            print(line)\n",
    "        line = line.strip().split(' ')\n",
    "        word = line[0]\n",
    "        embed = np.asarray(line[1:], \"float\")\n",
    "\n",
    "        embeddings_dict[word] = embed\n",
    "\n",
    "print('Loaded {} words from glove'.format(len(embeddings_dict)))\n",
    "\n",
    "embedding_matrix = np.zeros((len(embeddings_dict)+2, 100)) #add 1 for padding\n",
    "\n",
    "word2id = {}\n",
    "id2word = {}\n",
    "for i, word in enumerate(embeddings_dict.keys()):\n",
    "\n",
    "    word2id[word] = i                                #Map each word to an index\n",
    "    id2word[i] = word\n",
    "    embedding_matrix[i] = embeddings_dict[word]      #That index holds the Glove embedding in the embedding matrix\n",
    "\n",
    "# Our joint vocabulary for both models / sanity check to see if we've loaded it correctly:\n",
    "print(word2id['the'])\n",
    "print(embedding_matrix[word2id['the']])\n",
    "\n",
    "word2id['<pad>'] = embedding_matrix.shape[0] - 2\n",
    "word2id['<start>'] = embedding_matrix.shape[0] - 1\n",
    "id2word[embedding_matrix.shape[0] - 2] = '<pad>'\n",
    "id2word[embedding_matrix.shape[0] - 1] = '<start>'\n",
    "print(embedding_matrix[word2id['<pad>']])\n",
    "\n",
    "spam_full = pd.read_csv(\"./datasets/SpamHam/train.csv\")#, nrows=20000)\n",
    "\n",
    "split_ratio = 0.8\n",
    "train_size = int(split_ratio*spam_full.shape[0])\n",
    "spam_train = spam_full.iloc[:train_size]\n",
    "spam_valid = spam_full.iloc[train_size:]\n",
    "max_length = 120 #inclusive of start token\n",
    "'''\n",
    "train_size_url = int(split_ratio*urls_full.shape[0])\n",
    "urls_train = urls_full.iloc[:train_size]\n",
    "urls_valid = urls_full.iloc[train_size:]\n",
    "url_max_length = 120\n",
    "'''\n",
    "start_id = word2id['<start>']\n",
    "\n",
    "def tokenize_example(line):\n",
    "    example = [start_id]\n",
    "    tokenized = nltk.word_tokenize(line)\n",
    "    i = 0\n",
    "    for token in tokenized:\n",
    "        if not (token in word2id): continue #not using <unk> for spam dataset\n",
    "        i += 1\n",
    "        if (i >= max_length): break\n",
    "        example.append(word2id[token])\n",
    "        \n",
    "    #add padding\n",
    "    padding = word2id[\"<pad>\"]\n",
    "    for i in range(max_length - len(example)):\n",
    "        example.append(padding)\n",
    "    return np.array(example)\n",
    "\n",
    "def tokenize(df):\n",
    "    examples = []\n",
    "    for index, row in df.iterrows():\n",
    "        example = tokenize_example(row[\"text\"])\n",
    "        if (len(example) > 0 and len(example.shape) > 0): examples.append((example, row[\"label\"]))\n",
    "    return examples\n",
    "\n",
    "tokenized_spam_valid = tokenize(spam_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac439013-e425-40ac-93dd-e9acd1515e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_matrix, model_size, n_heads, n_layers, hidden_size, embedding_dims=100, vocab_size=None):\n",
    "        super().__init__()\n",
    "\n",
    "        if not (embedding_matrix is None): #glove\n",
    "            self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix))\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dims)\n",
    "        self.pos_encoding = PositionalEncoding(embedding_dims, max_length)\n",
    "        self.input_linear = nn.Linear(embedding_dims, model_size)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(model_size, n_heads, hidden_size, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layers, n_layers)\n",
    "        #self.encoder = nn.Transformer(encoder_layers, n_layers, batch_first=True)\n",
    "        self.output_hidden_1 = nn.Linear(model_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output_hidden_2 = nn.Linear(hidden_size, 2) #binary classification\n",
    "        self.model_size = model_size\n",
    "\n",
    "        #initialize\n",
    "        initrange = 0.1\n",
    "        self.input_linear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.input_linear.bias.data.zero_()\n",
    "        self.output_hidden_1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.output_hidden_1.bias.data.zero_()\n",
    "        self.output_hidden_2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.output_hidden_2.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        #print(\"intput.shape: \", input.shape, len(input.shape))\n",
    "        input = (self.embedding(input) * math.sqrt(self.model_size)) #recommended from documentation\n",
    "        input = self.pos_encoding(input)\n",
    "        #print(\"input after poe:\", input.shape)\n",
    "\n",
    "        input = self.input_linear(input) #get a representation that has the model size for the positionally encoded embeddings\n",
    "        #print(\"after input linear: \", input.shape)\n",
    "        \n",
    "        output = self.encoder(input)[:,0] #take the last vector\n",
    "        #print(\"after encoder: \", output.shape)\n",
    "        output = self.output_hidden_1(output)\n",
    "\n",
    "        output = self.relu(output)\n",
    "        output = self.output_hidden_2(output)\n",
    "\n",
    "        #print(\"after linear:\", output.shape)\n",
    "\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, model_size, max_len): #from torch documentation\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, model_size, 2) * (-math.log(10000.0) / model_size))\n",
    "        pe = torch.zeros(max_len, 1, model_size)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "def predict(model, valid_dataloader): #modified to save examples the model fails on\n",
    "\n",
    "    softmax = nn.Softmax()\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    sigmoid = nn.Sigmoid()\n",
    "\n",
    "    total_examples = 0\n",
    "    false_negative = []\n",
    "    false_positive = []\n",
    "\n",
    "    for x, y in valid_dataloader:\n",
    "        x = x.squeeze()\n",
    "        if (len(x.shape) == 0): continue\n",
    "        output = model(x)\n",
    "        output = softmax(output)\n",
    "\n",
    "        for i in range(output.shape[0]):\n",
    "            if (output[i][0].item() >= 0.5):\n",
    "                if (y[i].item() != 0):\n",
    "                    false_negative.append(untokenize(x[i]))\n",
    "            elif (y[i].item() == 0):\n",
    "                false_positive.append(untokenize(x[i]))\n",
    "        total_examples += output.shape[0]\n",
    "        #print(\"total examples:\", total_examples, \"; T+:\", true_positive, \"; F+:\", false_positive, \"; T-:\", true_negative, \"; F-:\", false_negative)\n",
    "\n",
    "    return [false_negative, false_positive]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a5259e-a37f-460b-9dae-ba365b6391df",
   "metadata": {},
   "source": [
    "We will also create a function that takes in word IDs and translates them back to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3589bbe5-f58a-4053-b44e-1f849717c3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def untokenize(example):\n",
    "    s = \"\"\n",
    "    for i in range(len(example)):\n",
    "        s += id2word[example[i].item()] + \" \"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6833b8bc-8489-428d-bedf-6ba43dc90510",
   "metadata": {},
   "source": [
    "## Spam Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9808f15a-8a32-483b-9b27-fde50132d775",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 6\n",
    "batch_size = 32\n",
    "print_frequency = 250\n",
    "n_heads = 2\n",
    "n_layers = 2\n",
    "model_size = 28\n",
    "hidden_size = 48\n",
    "pos_weight_coeff = 1.05\n",
    "lr = 0.001\n",
    "\n",
    "spam_model = TransformerModel(embedding_matrix, model_size=model_size, n_heads=n_heads, n_layers=n_layers, hidden_size=hidden_size)\n",
    "spam_model.load_state_dict(torch.load('./trained_models/spam.pt'))\n",
    "\n",
    "spam_valid_dataloader = torch.utils.data.DataLoader(tokenized_spam_valid, batch_size=batch_size)\n",
    "false_negative, false_positive = predict(spam_model, spam_valid_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3327818-d332-4eb2-92b4-e32bd71fc369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selected FALSE NEGATIVE examples: (427)\n",
      "Example 33: <start> two dynamic mlas harvey barry <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "\n",
      "\n",
      "Example 332: <start> take that <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "\n",
      "\n",
      "Example 65: <start> advertisement sent thestreet com currently within last year subscriber either free trial paid one web sites www thestreet com www com current former subscriber believe received message error please forward message members thestreet com call customer service department please assured respect privacy subscribers disclosed name information advertiser third party dear jeffrey register jeffrey com today secure piece web internet quickly becoming center communication invite register jeffrey com com com register com tm boom personal domain name registrations names going fast claim late http register com cgi bin wd asp name jeffrey com src whole package name addition personalized domain name package includes personalized email box example jeffrey jeffrey com choose web based email messages forwarded existing account multi page \n",
      "\n",
      "\n",
      "Example 337: <start> humphrey koch christian brogan hulme stewart zem <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "\n",
      "\n",
      "Example 354: <start> lesson building dont hes everything country principal <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "\n",
      "\n",
      "Example 308: <start> greetings from amazon com we 've recently learned from our supplier that the item you requested to be notified about little astronaut costume medium age will not be available in the foreseeable future it 's possible that someone may be selling this item through amazon com auctions or zshops we encourage you to search for it there if you 're still interested in purchasing this item http www amazon com auctions http www amazon com zshops we 're sorry not to have better news for you sincerely amazon com customer service http www amazon com <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Randomly selected FALSE NEGATIVE examples: (%s)\" % len(false_negative))\n",
    "for i in np.random.choice(np.arange(0, len(false_negative)), 6):\n",
    "    print(\"Example %s: %s\" % (i, false_negative[i]) + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3186f985-27aa-4ef9-9697-3c2b512e2da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selected FALSE POSITIVE examples: (608)\n",
      "Example 13: <start> test <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "\n",
      "\n",
      "Example 349: <start> is n't he just a cutie pie <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "\n",
      "\n",
      "Example 542: <start> sally , shari mao contacted her boyfriend and he gave her the name of a larynx specialist at methodist hospital - dr . donald t . donovan ( 713 - 798 - 5900 ) . i hope this might be helpful . lisa cousino <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "\n",
      "\n",
      "Example 134: <start> track manage your source code and digital assets for free with perforce discover how you can manage your source code and digital assets faster and easier with perforce perforce offers visual client interfaces for linux com mac freebsd and windows learn more and get a free two user download http sel as us net sel cmd lnk kid bid dat opt rdm timestamp slashdot daily newsletter in this issue red hat boosts selinux with rhel the apple ii at microsoft slaps its most valuable professional inkjet photo print longevity lacking memory checker tools for c nvidia 's andy on linux drivers at t ceo attacks network neutrality macbook pro gets santa rosa chipset led screen computex and gigabyte 's \n",
      "\n",
      "\n",
      "Example 529: <start> our records show that you did not select a conference track to attend please help us by updating your registration online please click on the link below and then click on the button labeled modify personal info to select which of the conference tracks you are most interested in attending your selection s will not confine you to a particular conference track but will assist in planning to make your experience most comfortable and enjoyable https www com asp id password note if clicking on the link above does not work please make sure you have internet access and then copy and paste this link into your browsers url if you have any questions please contact the group at \n",
      "\n",
      "\n",
      "Example 6: <start> the events of september are still very much with us and will continue to be for some time perhaps for as long as we live for those who have not heard me say this regarding air travel individuals should avail themselves of the travel services the company is offering in the way of guidance for those that are traveling and only travel by air for essential matters until further notice in addition anyone who is currently uncomfortable with the notion of air travel need not do so we will work around this for as long as the need exists the company has also provided information on the availability of counseling services should anyone have the need please contact hr \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Randomly selected FALSE POSITIVE examples: (%s)\" % len(false_positive))\n",
    "for i in np.random.choice(np.arange(0, len(false_positive)), 6):\n",
    "    print(\"Example %s: %s\" % (i, false_positive[i]) + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbc5e37-1e6c-4004-8125-977109c9001e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
