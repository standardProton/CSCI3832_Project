{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the trained models on our handmade dataset\n",
    "\n",
    "Alex Ludwigson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer's performance on the handmade dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 -0.48336 0.1464 -0.37304 0.34577 0.052041 0.44946 -0.46971 0.02628 -0.54155 -0.15518 -0.14107 -0.039722 0.28277 0.14393 0.23464 -0.31021 0.086173 0.20397 0.52624 0.17164 -0.082378 -0.71787 -0.41531 0.20335 -0.12763 0.41367 0.55187 0.57908 -0.33477 -0.36559 -0.54857 -0.062892 0.26584 0.30205 0.99775 -0.80481 -3.0243 0.01254 -0.36942 2.2167 0.72201 -0.24978 0.92136 0.034514 0.46745 1.1079 -0.19358 -0.074575 0.23353 -0.052062 -0.22044 0.057162 -0.15806 -0.30798 -0.41625 0.37972 0.15006 -0.53212 -0.2055 -1.2526 0.071624 0.70565 0.49744 -0.42063 0.26148 -1.538 -0.30223 -0.073438 -0.28312 0.37104 -0.25217 0.016215 -0.017099 -0.38984 0.87424 -0.72569 -0.51058 -0.52028 -0.1459 0.8278 0.27062\n",
      "\n",
      "Loaded 400000 words from glove\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'max_length' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 176\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m examples\n\u001b[0;32m    175\u001b[0m \u001b[38;5;66;03m#load the state dict \u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m transform \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m transform\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./trained_models/spam.pt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m#read the file\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 28\u001b[0m, in \u001b[0;36mTransformerModel.__init__\u001b[1;34m(self, embedding_matrix, model_size, n_heads, n_layers, hidden_size, embedding_dims, vocab_size)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(vocab_size, embedding_dims)\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoding \u001b[38;5;241m=\u001b[39m PositionalEncoding(embedding_dims, \u001b[43mmax_length\u001b[49m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_linear \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(embedding_dims, model_size)\n\u001b[0;32m     30\u001b[0m encoder_layers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mTransformerEncoderLayer(model_size, n_heads, hidden_size, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'max_length' is not defined"
     ]
    }
   ],
   "source": [
    "glove_file = \"./datasets/glove.6B.100d.txt\" #or 50d\n",
    "\n",
    "embeddings_dict = {}\n",
    "\n",
    "with open(glove_file, 'r', encoding='utf8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == 0:\n",
    "            print(line)\n",
    "        line = line.strip().split(' ')\n",
    "        word = line[0]\n",
    "        embed = np.asarray(line[1:], \"float\")\n",
    "\n",
    "        embeddings_dict[word] = embed\n",
    "\n",
    "print('Loaded {} words from glove'.format(len(embeddings_dict)))\n",
    "\n",
    "embedding_matrix = np.zeros((len(embeddings_dict)+2, 100)) #add 1 for padding\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_matrix, model_size, n_heads, n_layers, hidden_size, embedding_dims=100, vocab_size=None):\n",
    "        super().__init__()\n",
    "\n",
    "        if not (embedding_matrix is None): #glove\n",
    "            self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix))\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dims)\n",
    "        self.pos_encoding = PositionalEncoding(embedding_dims, max_length)\n",
    "        self.input_linear = nn.Linear(embedding_dims, model_size)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(model_size, n_heads, hidden_size, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layers, n_layers)\n",
    "        #self.encoder = nn.Transformer(encoder_layers, n_layers, batch_first=True)\n",
    "        self.output_hidden_1 = nn.Linear(model_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output_hidden_2 = nn.Linear(hidden_size, 2) #binary classification\n",
    "        self.model_size = model_size\n",
    "\n",
    "        #initialize\n",
    "        initrange = 0.1\n",
    "        self.input_linear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.input_linear.bias.data.zero_()\n",
    "        self.output_hidden_1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.output_hidden_1.bias.data.zero_()\n",
    "        self.output_hidden_2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.output_hidden_2.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        #print(\"intput.shape: \", input.shape, len(input.shape))\n",
    "        input = (self.embedding(input) * math.sqrt(self.model_size)) #recommended from documentation\n",
    "        input = self.pos_encoding(input)\n",
    "        #print(\"input after poe:\", input.shape)\n",
    "\n",
    "        input = self.input_linear(input) #get a representation that has the model size for the positionally encoded embeddings\n",
    "        #print(\"after input linear: \", input.shape)\n",
    "        \n",
    "        output = self.encoder(input)[:,0] #take the last vector\n",
    "        #print(\"after encoder: \", output.shape)\n",
    "        output = self.output_hidden_1(output)\n",
    "\n",
    "        output = self.relu(output)\n",
    "        output = self.output_hidden_2(output)\n",
    "\n",
    "        #print(\"after linear:\", output.shape)\n",
    "\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, model_size, max_len): #from torch documentation\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, model_size, 2) * (-math.log(10000.0) / model_size))\n",
    "        pe = torch.zeros(max_len, 1, model_size)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "#Define hyperparameters\n",
    "epochs = 6\n",
    "batch_size = 32\n",
    "print_frequency = 250\n",
    "n_heads = 2\n",
    "n_layers = 2\n",
    "model_size = 28\n",
    "hidden_size = 48\n",
    "pos_weight_coeff = 1.05\n",
    "\n",
    "\n",
    "\n",
    "max_length = 120 #inclusive of start token\n",
    "start_id = word2id['<start>']\n",
    "\n",
    "def predict(model, valid_dataloader):\n",
    "\n",
    "    softmax = nn.Softmax()\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    sigmoid = nn.Sigmoid()\n",
    "\n",
    "    total_examples = 0\n",
    "    total_positive = 0\n",
    "    total_negative = 0\n",
    "\n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    true_negative = 0\n",
    "    false_negative = 0\n",
    "\n",
    "    for x, y in valid_dataloader:\n",
    "        x = x.squeeze()\n",
    "        if (len(x.shape) == 0): continue\n",
    "        output = model(x)\n",
    "        output = softmax(output)\n",
    "\n",
    "        for i in range(output.shape[0]):\n",
    "            if (output[i][0].item() >= 0.5):\n",
    "                if (y[i].item() == 0):\n",
    "                    true_negative += 1\n",
    "                    total_negative += 1\n",
    "                else:\n",
    "                    false_negative += 1\n",
    "                    total_positive += 1\n",
    "            else:\n",
    "                if (y[i].item() == 0):\n",
    "                    false_positive += 1\n",
    "                    total_negative += 1\n",
    "                else:\n",
    "                    true_positive += 1\n",
    "                    total_positive += 1\n",
    "        total_examples += output.shape[0]\n",
    "        #print(\"total examples:\", total_examples, \"; T+:\", true_positive, \"; F+:\", false_positive, \"; T-:\", true_negative, \"; F-:\", false_negative)\n",
    "\n",
    "    accuracy = (true_positive + true_negative) / total_examples\n",
    "    t_p = true_positive/total_examples\n",
    "    f_p = false_positive/total_examples\n",
    "    t_n = true_negative/total_examples\n",
    "    f_n = false_negative/total_examples\n",
    "    p = true_positive/(true_positive + false_positive)\n",
    "    r = true_positive/(true_positive + false_negative)\n",
    "    f_score = (2*p*r)/(p+r)\n",
    "\n",
    "    print('accuracy: %s/%s = %s' % (true_positive+true_negative, total_examples, (true_positive + true_negative) / total_examples))\n",
    "    print('True positive: %s' % t_p)\n",
    "    print(\"False positive: %s\" % f_p)\n",
    "    print('True negative: %s' % t_n)\n",
    "    print(\"False negative: %s\" % f_n)\n",
    "    print(\"(P, R, F-Score) = (%s, %s, %s)\\n\" % (p, r, f_score))\n",
    "    return accuracy\n",
    "\n",
    "def tokenize_example(line):\n",
    "    example = [start_id]\n",
    "    tokenized = nltk.word_tokenize(line)\n",
    "    i = 0\n",
    "    for token in tokenized:\n",
    "        if not (token in word2id): continue #not using <unk> for spam dataset\n",
    "        i += 1\n",
    "        if (i >= max_length): break\n",
    "        example.append(word2id[token])\n",
    "        \n",
    "    #add padding\n",
    "    padding = word2id[\"<pad>\"]\n",
    "    for i in range(max_length - len(example)):\n",
    "        example.append(padding)\n",
    "    return np.array(example)\n",
    "\n",
    "def tokenize(df):\n",
    "    examples = []\n",
    "    for index, row in df.iterrows():\n",
    "        example = tokenize_example(row[\"text\"])\n",
    "        if (len(example) > 0 and len(example.shape) > 0): examples.append((example, row[\"label\"]))\n",
    "    return examples\n",
    "\n",
    "#load the state dict \n",
    "transform = TransformerModel(embedding_matrix, model_size=model_size, n_heads=n_heads, n_layers=n_layers, hidden_size=hidden_size)\n",
    "transform.load_state_dict(torch.load('./trained_models/spam.pt'))\n",
    "#read the file\n",
    "spam_hm = pd.read_csv(\"./HomebrewDataset.csv\")\n",
    "spam_hm_tok = tokenize(spam_hm)\n",
    "spam_valid_dataloader = torch.utils.data.DataLoader(spam_hm_tok, batch_size=batch_size)\n",
    "predict(transform, spam_valid_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert's performance on the handmade dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT2's performance on the handmade dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM's performance on the handmade dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive approach's performance on handmade dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
